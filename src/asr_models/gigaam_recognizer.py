import os
import time
import wave
import asyncio
from typing import Optional, List
from collections import deque
import numpy as np
import onnxruntime as rt
from asr_models.speech_recognizer_base import SpeechRecognizerInterface
from utils import getTranslationVariant as _
from utils.gpu_utils import check_gpu_provider

def load_my_onnx_sessions(
    onnx_dir: str,
    model_version: str,
    providers,
) -> List[rt.InferenceSession]:
    if isinstance(providers, str):
        providers = [providers]
    else:
        providers = list(providers)
    if "CPUExecutionProvider" not in providers:
        providers.append("CPUExecutionProvider")

    if "_" in model_version:
        version, model_type = model_version.split("_", 1)
    else:
        version, model_type = "v2", model_version

    opts = rt.SessionOptions()
    opts.intra_op_num_threads = 16
    opts.execution_mode = rt.ExecutionMode.ORT_SEQUENTIAL

    sessions: List[rt.InferenceSession] = []

    if model_type == "ctc":
        model_path = os.path.join(onnx_dir, f"{version}_{model_type}.onnx")
        sessions.append(rt.InferenceSession(model_path,
                                            providers=providers,
                                            sess_options=opts))
    else:
        base = os.path.join(onnx_dir, f"{version}_{model_type}")
        for part in ("encoder", "decoder", "joint"):
            path = f"{base}_{part}.onnx"
            sessions.append(rt.InferenceSession(path,
                                                providers=providers,
                                                sess_options=opts))
    return sessions

class GigaAMRecognizer(SpeechRecognizerInterface):
    def __init__(self, pip_installer, logger):
        super().__init__(pip_installer, logger)
        self._torch = None
        self._gigaam = None
        self._onnx_runtime = None
        self._gigaam_model_instance = None
        self._gigaam_onnx_sessions = None
        self._current_gpu = None
        self.gigaam_model = "v2_rnnt"
        self.gigaam_device = "auto"
        self.gigaam_onnx_export_path = "SpeechRecognitionModels/GigaAM_ONNX"
        self.FAILED_AUDIO_DIR = "FailedAudios"
        
    def _show_install_warning(self, packages: list):
        package_str = ", ".join(packages)
        self.logger.warning("="*80)
        self.logger.warning(_(
            f"Ð’ÐÐ˜ÐœÐÐÐ˜Ð•: Ð”Ð»Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ð²Ñ‹Ð±Ñ€Ð°Ð½Ð½Ð¾Ð³Ð¾ Ð¼Ð¾Ð´ÑƒÐ»Ñ Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ñ Ñ€ÐµÑ‡Ð¸ Ñ‚Ñ€ÐµÐ±ÑƒÑŽÑ‚ÑÑ Ð±Ð¸Ð±Ð»Ð¸Ð¾Ñ‚ÐµÐºÐ¸: {package_str}.",
            f"WARNING: The selected speech recognition module requires libraries: {package_str}."
        ))
        self.logger.warning(_(
            "Ð¡ÐµÐ¹Ñ‡Ð°Ñ Ð½Ð°Ñ‡Ð½ÐµÑ‚ÑÑ Ð¸Ñ… Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ°Ñ ÑƒÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ°. Ð­Ñ‚Ð¾ Ð¼Ð¾Ð¶ÐµÑ‚ Ð·Ð°Ð½ÑÑ‚ÑŒ Ð½ÐµÐºÐ¾Ñ‚Ð¾Ñ€Ð¾Ðµ Ð²Ñ€ÐµÐ¼Ñ.",
            "Automatic installation will now begin. This may take some time."
        ))
        self.logger.warning(_(
            "Ð¢Ð°ÐºÐ¶Ðµ, Ð¿Ð¾ÑÐ»Ðµ ÑƒÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ¸, Ð±ÑƒÐ´ÐµÑ‚ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð° Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ñ, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ð¼Ð¾Ð¶ÐµÑ‚ Ð·Ð°Ð½Ð¸Ð¼Ð°Ñ‚ÑŒ Ð´Ð¾ 1 Ð“Ð‘ Ð´Ð¸ÑÐºÐ¾Ð²Ð¾Ð³Ð¾ Ð¿Ñ€Ð¾ÑÑ‚Ñ€Ð°Ð½ÑÑ‚Ð²Ð°.",
            "Also, after installation, a recognition model will be downloaded, which can take up to 1 GB of disk space."
        ))
        self.logger.warning("="*80)
        time.sleep(3)
        
    def set_options(self, device: str, model: str = None, onnx_path: str = None):
        self.gigaam_device = device
        if model:
            self.gigaam_model = model
        if onnx_path:
            self.gigaam_onnx_export_path = onnx_path
        self.logger.info(f"Ð£ÑÑ‚Ñ€Ð¾Ð¹ÑÑ‚Ð²Ð¾ Ð´Ð»Ñ GigaAM ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¾ Ð½Ð°: {device}")
    
    async def install(self) -> bool:
        try:
            if self._torch is None:
                try:
                    import torch
                except ImportError:
                    if self._current_gpu is None:
                        self._current_gpu = check_gpu_provider() or "CPU"
                    
                    if self._current_gpu == "NVIDIA":
                        success = self.pip_installer.install_package(
                            ["torch==2.7.1", "torchaudio==2.7.1"],
                            description=_("Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° PyTorch Ñ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ¾Ð¹ CUDA...", "Installing PyTorch with CUDA support..."),
                            extra_args=["--index-url", "https://download.pytorch.org/whl/cu128"]
                        )
                    else:
                        success = self.pip_installer.install_package(
                            ["torch==2.7.1", "torchaudio==2.7.1"],
                            description=_("Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° PyTorch CPU...", "Installing PyTorch CPU..."),
                        )
                    if not success:
                        raise ImportError("ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ torch, Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ñ‹Ð¹ Ð´Ð»Ñ GigaAM.")
                    import torch
                try:
                    import omegaconf
                except ImportError:
                    success = self.pip_installer.install_package(
                        "omegaconf",
                        description=_("Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° omegaconf...", "Installing omegaconf...")
                    )
                    if not success:
                        raise ImportError("ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ omegaconf, Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ñ‹Ð¹ Ð´Ð»Ñ GigaAM.")
                
                import omegaconf, typing, collections
                torch.serialization.add_safe_globals([omegaconf.dictconfig.DictConfig, omegaconf.base.ContainerMetadata, typing.Any, dict, collections.defaultdict, omegaconf.nodes.AnyNode, omegaconf.nodes.Metadata, omegaconf.listconfig.ListConfig, list, int])
                self.logger.warning("TORCH ADDED SAFE GLOBALS!")
                self._torch = torch

            if self._gigaam is None:
                try:
                    import gigaam
                except ImportError:
                    self._show_install_warning(["gigaam"])
                    success = self.pip_installer.install_package(
                        ["gigaam", "hydra-core", "sentencepiece"],
                        description=_("Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° GigaAM...", "Installing GigaAM..."),
                        extra_args=["--no-deps"]
                    )
                    if not success:
                        raise ImportError("ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ GigaAM.")
                    import gigaam
                self._gigaam = gigaam
            return True
        except ImportError as e:
            self.logger.critical(f"ÐšÑ€Ð¸Ñ‚Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ°: Ð½Ðµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¸Ð¼Ð¿Ð¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¸Ð»Ð¸ ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Ð±Ð¸Ð±Ð»Ð¸Ð¾Ñ‚ÐµÐºÑƒ Ð´Ð»Ñ GigaAM: {e}")
            return False
    
    async def init(self, **kwargs) -> bool:
        if self._gigaam_model_instance is not None or self._gigaam_onnx_sessions is not None:
            return True

        if self._gigaam is None:
            self.logger.error("ÐœÐ¾Ð´ÑƒÐ»ÑŒ GigaAM Ð½Ðµ Ð±Ñ‹Ð» Ð¸Ð¼Ð¿Ð¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½. Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð½ÐµÐ²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð°.")
            return False
        
        if self._current_gpu is None:
            self._current_gpu = check_gpu_provider() or "CPU"

        device_choice = self.gigaam_device
        is_nvidia = self._current_gpu == "NVIDIA"

        if is_nvidia and device_choice in ["auto", "cuda", "cpu"]:
            device = "cuda" if device_choice in ["auto", "cuda"] else "cpu"
            self.logger.info(f"Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ GigaAM Ð´Ð»Ñ NVIDIA Ð½Ð° ÑƒÑÑ‚Ñ€Ð¾Ð¹ÑÑ‚Ð²Ðµ: {device}")
            self.logger.warning(_(
                f"ÐÐ°Ñ‡Ð¸Ð½Ð°ÐµÑ‚ÑÑ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¼Ð¾Ð´ÐµÐ»Ð¸ GigaAM ({self.gigaam_model}). Ð Ð°Ð·Ð¼ÐµÑ€ ~1 Ð“Ð‘. ÐŸÐ¾Ð¶Ð°Ð»ÑƒÐ¹ÑÑ‚Ð°, Ð¿Ð¾Ð´Ð¾Ð¶Ð´Ð¸Ñ‚Ðµ...",
                f"Starting download of GigaAM model ({self.gigaam_model}). Size ~1 GB. Please wait..."
            ))
            try:
                model = self._gigaam.load_model(self.gigaam_model, device=device)
                self._gigaam_model_instance = model
                self.logger.info(f"ÐœÐ¾Ð´ÐµÐ»ÑŒ GigaAM '{self.gigaam_model}' ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð° Ð½Ð° {device}.")
                self._is_initialized = True
                return True
            except Exception as e:
                self.logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ PyTorch Ð¼Ð¾Ð´ÐµÐ»Ð¸ GigaAM: {e}", exc_info=True)
                return False
        else:
            provider = 'CPUExecutionProvider'
            if device_choice in ["auto", "dml"] and not is_nvidia:
                provider = 'DmlExecutionProvider'
            
            self.logger.info(f"Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ GigaAM Ñ‡ÐµÑ€ÐµÐ· ONNX Ñ Ð¿Ñ€Ð¾Ð²Ð°Ð¹Ð´ÐµÑ€Ð¾Ð¼: {provider}")

            if self._onnx_runtime is None:
                try:
                    import onnxruntime
                except ImportError:
                    deps_to_install = ["onnx", "onnxruntime"]
                    desc = _("Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° ONNX Runtime...", "Installing ONNX Runtime...")
                    if provider == 'DmlExecutionProvider':
                        deps_to_install.append("onnxruntime-directml")
                        desc = _("Ð£ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ° ONNX Runtime Ñ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ¾Ð¹ DirectML...", "Installing ONNX Runtime with DirectML support...")
                    
                    self._show_install_warning(deps_to_install)
                    self.pip_installer.install_package(deps_to_install, description=desc)
                    import onnxruntime
                self._onnx_runtime = onnxruntime

            onnx_dir = self.gigaam_onnx_export_path
            os.makedirs(onnx_dir, exist_ok=True)
            
            encoder_path = os.path.join(onnx_dir, f"{self.gigaam_model}_encoder.onnx")

            if not os.path.exists(encoder_path):
                self.logger.warning(_(
                    f"ÐœÐ¾Ð´ÐµÐ»ÑŒ GigaAM ONNX Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°. ÐÐ°Ñ‡Ð¸Ð½Ð°ÐµÑ‚ÑÑ ÑÐºÑÐ¿Ð¾Ñ€Ñ‚. Ð­Ñ‚Ð¾ ÐµÐ´Ð¸Ð½Ð¾Ñ€Ð°Ð·Ð¾Ð²Ñ‹Ð¹ Ð¿Ñ€Ð¾Ñ†ÐµÑÑ, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð¿Ð¾Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ ~1 Ð“Ð‘ Ð¸ Ð¼Ð¾Ð¶ÐµÑ‚ Ð·Ð°Ð½ÑÑ‚ÑŒ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ Ð¼Ð¸Ð½ÑƒÑ‚.",
                    f"GigaAM ONNX model not found. Exporting will now begin. This is a one-time process that will download ~1GB and may take several minutes."
                ))
                try:
                    temp_model = self._gigaam.load_model(
                        self.gigaam_model,
                        device="cpu",
                        fp16_encoder=False,
                        use_flash=False
                    )
                    temp_model.to_onnx(dir_path=onnx_dir)
                    del temp_model
                    if self._torch.cuda.is_available():
                        self._torch.cuda.empty_cache()
                    self.logger.info(f"ÐœÐ¾Ð´ÐµÐ»ÑŒ GigaAM ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ ÑÐºÑÐ¿Ð¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð° Ð² ONNX Ð² Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸ÑŽ: {onnx_dir}")
                except Exception as e:
                    self.logger.error(f"ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ ÑÐºÑÐ¿Ð¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¼Ð¾Ð´ÐµÐ»ÑŒ GigaAM Ð² ONNX: {e}", exc_info=True)
                    return False

            try:
                self.logger.info(f"Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° ONNX ÑÐµÑÑÐ¸Ð¹ Ð¸Ð· {onnx_dir}...")
                sessions = load_my_onnx_sessions(
                    onnx_dir,
                    self.gigaam_model,
                    provider
                )
                self._gigaam_onnx_sessions = sessions
                self.logger.info(f"ONNX ÑÐµÑÑÐ¸Ð¸ Ð´Ð»Ñ GigaAM ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ñ‹ Ñ Ð¿Ñ€Ð¾Ð²Ð°Ð¹Ð´ÐµÑ€Ð¾Ð¼ {provider}.")
                self._is_initialized = True
                return True
            except Exception as e:
                self.logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ ONNX ÑÐµÑÑÐ¸Ð¹ GigaAM: {e}", exc_info=True)
                return False
    
    async def transcribe(self, audio_data: np.ndarray, sample_rate: int) -> Optional[str]:
        if not self._is_initialized:
            return None
            
        pytorch_model = self._gigaam_model_instance
        onnx_sessions = self._gigaam_onnx_sessions

        if pytorch_model is None and onnx_sessions is None:
            self.logger.error("Ð Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ñ‚ÐµÐ»ÑŒ GigaAM Ð½Ðµ Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½.")
            return None
        
        from gigaam.onnx_utils import transcribe_sample

        TEMP_AUDIO_DIR = "TempAudios"
        os.makedirs(TEMP_AUDIO_DIR, exist_ok=True)
        temp_filepath = os.path.join(TEMP_AUDIO_DIR, f"temp_gigaam_{time.time_ns()}.wav")
        
        try:
            audio_data_int16 = (audio_data * 32767).astype(np.int16)
            with wave.open(temp_filepath, 'wb') as wf:
                wf.setnchannels(1)
                wf.setsampwidth(2)
                wf.setframerate(sample_rate)
                wf.writeframes(audio_data_int16.tobytes())

            transcription = ""
            if pytorch_model:
                transcription = pytorch_model.transcribe(temp_filepath)
            elif onnx_sessions:
                model_type = self.gigaam_model.split("_", 1)[-1]
                transcription = transcribe_sample(
                        temp_filepath,
                        model_type,
                        onnx_sessions
                )

            if transcription and transcription.strip() != '':
                return transcription
            else:
                self.logger.info("GigaAM Ð½Ðµ Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð» Ñ‚ÐµÐºÑÑ‚.")
                return None

        except Exception as e:
            self.logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð²Ð¾ Ð²Ñ€ÐµÐ¼Ñ Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ñ GigaAM: {e}", exc_info=True)
            return None

        finally:
            if os.path.exists(temp_filepath):
                try:
                    os.remove(temp_filepath)
                except OSError as e:
                    self.logger.error(f"ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ ÑƒÐ´Ð°Ð»Ð¸Ñ‚ÑŒ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ð¹ Ñ„Ð°Ð¹Ð» {temp_filepath}: {e}")
    
    async def live_recognition(self, microphone_index: int, handle_voice_callback, 
                              vad_model, active_flag, **kwargs) -> None:
        try:
            import sounddevice as sd
            import torch
        except ImportError as e:
            self.logger.error(f"ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¸Ð¼Ð¿Ð¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ñ‹Ðµ Ð±Ð¸Ð±Ð»Ð¸Ð¾Ñ‚ÐµÐºÐ¸: {e}")
            return
        
        sample_rate = kwargs.get('sample_rate', 16000)
        chunk_size = kwargs.get('chunk_size', 512)
        vad_threshold = kwargs.get('vad_threshold', 0.5)
        silence_timeout = kwargs.get('silence_timeout', 1.0)
        pre_buffer_duration = kwargs.get('pre_buffer_duration', 0.3)
        
        silence_chunks_needed = int(silence_timeout * sample_rate / chunk_size)
        pre_buffer_size = int(pre_buffer_duration * sample_rate / chunk_size)
        
        try:
            devices = sd.query_devices()
            input_devices = [dev['name'] for dev in devices if dev['max_input_channels'] > 0]
            mic_name = input_devices[microphone_index] if microphone_index < len(input_devices) else "Unknown"
            self.logger.info(f"Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½: {mic_name}")
        except Exception as e:
            self.logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ð¸ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸ Ð¾ Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½Ðµ: {e}")
            return

        self.logger.info("ÐžÐ¶Ð¸Ð´Ð°Ð½Ð¸Ðµ Ñ€ÐµÑ‡Ð¸ (GigaAM + Silero VAD Ñ Ð¿Ñ€ÐµÐ´-Ð±ÑƒÑ„ÐµÑ€Ð¸Ð·Ð°Ñ†Ð¸ÐµÐ¹)...")

        pre_speech_buffer = deque(maxlen=pre_buffer_size)
        speech_buffer = []
        is_speaking = False
        silence_counter = 0

        with sd.InputStream(
            samplerate=sample_rate,
            channels=1,
            dtype='float32',
            blocksize=chunk_size,
            device=microphone_index
        ) as stream:
            while active_flag():
                audio_chunk, overflowed = stream.read(chunk_size)
                if overflowed:
                    self.logger.warning("ÐŸÐµÑ€ÐµÐ¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ Ð±ÑƒÑ„ÐµÑ€Ð° Ð°ÑƒÐ´Ð¸Ð¾Ð¿Ð¾Ñ‚Ð¾ÐºÐ°!")

                if not is_speaking:
                    pre_speech_buffer.append(audio_chunk)

                audio_tensor = torch.from_numpy(audio_chunk.flatten())
                speech_prob = vad_model(audio_tensor, sample_rate).item()

                if speech_prob > vad_threshold:
                    if not is_speaking:
                        self.logger.debug("ðŸŸ¢ ÐÐ°Ñ‡Ð°Ð»Ð¾ Ñ€ÐµÑ‡Ð¸. Ð—Ð°Ñ…Ð²Ð°Ñ‚ Ð¸Ð· Ð¿Ñ€ÐµÐ´-Ð±ÑƒÑ„ÐµÑ€Ð°.")
                        is_speaking = True
                        speech_buffer.clear()
                        speech_buffer.extend(list(pre_speech_buffer))
                    
                    speech_buffer.append(audio_chunk)
                    silence_counter = 0
                
                elif is_speaking:
                    speech_buffer.append(audio_chunk)
                    silence_counter += 1
                    if silence_counter > silence_chunks_needed:
                        self.logger.debug("ðŸ”´ ÐšÐ¾Ð½ÐµÑ† Ñ€ÐµÑ‡Ð¸. ÐžÑ‚Ð¿Ñ€Ð°Ð²ÐºÐ° Ð½Ð° Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ðµ.")
                        audio_to_process = np.concatenate(speech_buffer)
                        
                        is_speaking = False
                        speech_buffer.clear()
                        silence_counter = 0
                        
                        text = await self.transcribe(audio_to_process, sample_rate)
                        if text:
                            self.logger.info(f"GigaAM Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð»: {text}")
                            await handle_voice_callback(text)
                        else:
                            await self._save_failed_audio(audio_to_process, sample_rate)
                
                await asyncio.sleep(0.01)
    
    async def _save_failed_audio(self, audio_data: np.ndarray, sample_rate: int):
        self.logger.info("Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð°ÑƒÐ´Ð¸Ð¾Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚Ð° Ð² Ð¿Ð°Ð¿ÐºÑƒ Failed...")
        try:
            os.makedirs(self.FAILED_AUDIO_DIR, exist_ok=True)
            timestamp = int(time.time())
            filename = os.path.join(self.FAILED_AUDIO_DIR, f"failed_{timestamp}.wav")
            
            audio_data_int16 = (audio_data * 32767).astype(np.int16)

            with wave.open(filename, 'wb') as wf:
                wf.setnchannels(1)
                wf.setsampwidth(2)
                wf.setframerate(sample_rate)
                wf.writeframes(audio_data_int16.tobytes())
            self.logger.info(f"Ð¤Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½ Ð²: {filename}")
        except Exception as e:
            self.logger.error(f"ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ ÑÐ¾Ñ…Ñ€Ð°Ð½Ð¸Ñ‚ÑŒ Ð°ÑƒÐ´Ð¸Ð¾Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚: {e}")
    
    def cleanup(self) -> None:
        self._torch = None
        self._gigaam = None
        self._onnx_runtime = None
        self._gigaam_model_instance = None
        self._gigaam_onnx_sessions = None
        self._is_initialized = False